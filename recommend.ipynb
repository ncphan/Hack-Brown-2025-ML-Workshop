{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Song Recommender Program Using Unsupervised Learning through the K-Nearest Neighbors Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll want to create a virtual environment in Python. We want to do this because virtual environments essentially give you a way to isolate your dependencies from other Python programs. Essentially, since we'll be installing and using Python packages in this demo, we don't want these packages to interfere with any other Python programs that you may have, so we'll just create our own virtual environment for this. Run the two commands below in the terminal to setup a virtual environment.\n",
    "\n",
    "-   python3 -m venv myenv\n",
    "\n",
    "-   source myenv/bin/activate\n",
    "\n",
    "Then, install pandas and scikit-learn (which are packages that we'll be using) by running the two commands below and wait for it to download.\n",
    "\n",
    "-   pip install pandas\n",
    "\n",
    "-   pip install scikit-learn\n",
    "\n",
    "After that, you may need to select a kernel. Do this by clicking the 'kernel' button in the top-right and setting it to your virtual environment. You may need to install ipykernel to do this, which you should do.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas\n",
    "import pandas as pd\n",
    " \n",
    "# scikit-learn imports\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, our steps that we'll take are as follows:\n",
    "\n",
    "1. Read in the dataset and convert it to a pandas dataframe.\n",
    "\n",
    "2. Explore the dataset by looking at its dimensions, what columns it has, etc.\n",
    "\n",
    "3. Clean the dataset by getting rid of any duplicates, missing values, and unneeded columns.\n",
    "\n",
    "4. Convert categorical data into numerical data that can be used for our algorithm.\n",
    "\n",
    "5. Scale the data to minimize biases.\n",
    "\n",
    "6. Create the KNN model and ask the user for an input track."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset was taken from Kaggle at https://www.kaggle.com/datasets/maharshipandya/-spotify-tracks-dataset. It contains data for over 100,000 tracks across over 125 different genres. It was last updated 2 years ago, so the program won't be able to accept songs released after then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the csv file and create a dataframe\n",
    "tracks = pd.read_csv('dataset.csv')\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the first 5 rows\n",
    "tracks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out dimensions of df (rows, columns)\n",
    "tracks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the columns to see what attributes each track has\n",
    "tracks.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may have noticed, the quality of our machine learning model is heavily reliant on our dataset. Therefore, we need to make sure that our dataset is clean in order to produce high-quality results. This includes dealing with missing/NULL values, duplicate data, and irrelevant data in our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out if we have any duplicates in our dataset. Looks like there's not!\n",
    "tracks.duplicated()\n",
    "\n",
    "# see if there's missing data in any of the columns\n",
    "tracks.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the columns into categorical and numerical data\n",
    "\n",
    "# categorical columns\n",
    "cat_col = [col for col in tracks.columns if tracks[col].dtype == 'object']\n",
    "print('Categorical columns :',cat_col)\n",
    "# numerical columns\n",
    "num_col = [col for col in tracks.columns if tracks[col].dtype != 'object']\n",
    "print('Numerical columns :',num_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we see that there are some rows with the same track ID. This means that we have duplicate songs!\n",
    "tracks[cat_col].nunique()\n",
    "\n",
    "unique_tracks = tracks.drop_duplicates().reset_index(drop=True)\n",
    "duplicates = unique_tracks['track_id'].duplicated()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still have some duplicates because some songs are remixes, so they have different track IDs. Thus, it makes it harder to find these remixes and get rid of them. You could probably utilize regex's to do this, but for the purposes of this workshop, we'll just leave in the remixes. They shouldn't affect the model that much since there are over 100,000 songs in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove a feature that we aren't interested in\n",
    "unique_tracks = unique_tracks.drop(['track_id'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order for our machine learning algorithm to work, we'll need each categorical variable to be converted into real numbers. We can do this by using label encoding, which assigns each unique genre a number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the unique genres\n",
    "unique_genres = unique_tracks[\"track_genre\"].unique()\n",
    "print(unique_genres)\n",
    "\n",
    "# TODO: create a LabelEncoder() and then use it to fit and transform the 'track_genre' data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'unique_tracks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# show the dataframe with the encoded genre\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43munique_tracks\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrack_genre\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39munique()\n\u001b[1;32m      3\u001b[0m non_unique_rows \u001b[38;5;241m=\u001b[39m unique_tracks[unique_tracks\u001b[38;5;241m.\u001b[39mduplicated(keep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)]\n\u001b[1;32m      4\u001b[0m non_unique_rows\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'unique_tracks' is not defined"
     ]
    }
   ],
   "source": [
    "# show the dataframe with the encoded genre\n",
    "unique_tracks[\"track_genre\"].unique()\n",
    "non_unique_rows = unique_tracks[unique_tracks.duplicated(keep=False)]\n",
    "non_unique_rows.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'unique_tracks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# let's also get rid of songs with duplicate track names\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m unique_tracks \u001b[38;5;241m=\u001b[39m \u001b[43munique_tracks\u001b[49m\u001b[38;5;241m.\u001b[39mdrop_duplicates(subset\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrack_name\u001b[39m\u001b[38;5;124m'\u001b[39m], keep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfirst\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m non_unique_rows \u001b[38;5;241m=\u001b[39m unique_tracks[unique_tracks\u001b[38;5;241m.\u001b[39mduplicated(keep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)]\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(non_unique_rows))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'unique_tracks' is not defined"
     ]
    }
   ],
   "source": [
    "# let's also get rid of songs with duplicate track names\n",
    "\n",
    "unique_tracks = unique_tracks.drop_duplicates(subset=['track_name'], keep='first').reset_index(drop=True)\n",
    "non_unique_rows = unique_tracks[unique_tracks.duplicated(keep=False)]\n",
    "\n",
    "print(len(non_unique_rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before with the 'track_genre' feature, we have another feature, the 'explicit' feature, which represents a boolean that is True if the song is explicit and False if not. Here, we can use one-hot encoding to convert each value to a 0 or a 1 to work with our algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: utilize one-hot encoding to convert the 'explicit' feature into a binary format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_columns():\n",
    "    '''In this function, we'll be scaling the data by standardizing it, making sure that it's a dataframe,\n",
    "    printing its head and shape, and then returning the scaled dataframe.'''\n",
    "    \n",
    "    # TODO: create a StandardScaler() object, fit the scaler to the numerical data and transform the data, \n",
    "    # convert to dataframe\n",
    "\n",
    "\n",
    "    # TODO: print the head and shape of the new dataframe\n",
    "    \n",
    "    \n",
    "    return scaled_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_knn(scaled_df):\n",
    "    '''Initializes the K-Nearest Neighbors model and fits it on the given scaled dataframe.'''\n",
    "    \n",
    "    # TODO: create the model, letting k = 10 and the distance formula be 'euclidean'. Then, fit it to the data\n",
    "    \n",
    "    # knn = ??\n",
    "\n",
    "    return knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_songs(track, knn, scaled_data):\n",
    "    '''Given an input track, checks if the track is in the dataset. If so, it retrieves the row number of its first\n",
    "    appearance and uses the K-Nearest Neighbors model to find the 10 nearest neighbors. Then, it prints the 10 tracks\n",
    "    and corresponding artists.'''\n",
    "    \n",
    "\n",
    "    if track not in unique_tracks['track_name'].values:\n",
    "        print(\"Track not in dataset\")\n",
    "    else:\n",
    "        row_index = unique_tracks[unique_tracks['track_name'] == track].index[0]\n",
    "        unique_tracks.iloc[row_index]\n",
    "        \n",
    "        distances, indices = knn.kneighbors(scaled_data.iloc[row_index].values.reshape(1, -1))\n",
    "\n",
    "        ret_tracks = []\n",
    "        # for each of the 10 closest tracks, append the 'track_name' and 'artists' features to the list\n",
    "        for i in indices:\n",
    "            ret_tracks.append((unique_tracks.iloc[i]['track_name'], unique_tracks.iloc[i]['artists']))\n",
    "        print(ret_tracks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's call our functions and ask the user to input a track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: uncomment the below lines and find out what functions should go in the question marks\n",
    "\n",
    "# scaled_data = ??\n",
    "# knn = ??\n",
    "input_track = input(\"Enter a track: \")\n",
    "recommend_songs(input_track, knn, scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you're done with running the project, feel free to deactivate your virtual environment by running the following command in the terminal:\n",
    "\n",
    "- deactivate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
